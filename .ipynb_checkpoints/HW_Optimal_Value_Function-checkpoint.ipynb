{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3aa0763",
   "metadata": {},
   "source": [
    "# HW - Optimal Value Functions\n",
    "<table style=\"margin-left: 0px; margin-top: 20px; margin-bottom: 20px;\">\n",
    "<tr>\n",
    "<td style=\"width:120px; padding-top: 10px; padding-bottom: 10px;\"><img src=\"assets/logo_ipparis.png\" style=\"height: 130px;\"></td>\n",
    "<td style=\"padding-left: 12px;\">\n",
    "<table style=\"width: 100%;\">\n",
    "<tr>\n",
    "<th style=\"text-align: left; width: 80px;\">File</th>\n",
    "<td style=\"text-align: left;\">HW_Optimal_Value_Function.ipynb</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th style=\"text-align: left;\">Author</th>\n",
    "<td style=\"text-align: left;\">Jiwon KANG - Sara NARANJO</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th style=\"text-align: left;\">Affiliation</th>\n",
    "<td style=\"text-align: left;\">Institut Polytechnique de Paris &nbsp;|&nbsp; Telecom SudParis &nbsp;</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th style=\"text-align: left;\">Date</th>\n",
    "<td style=\"text-align: left;\">December 2, 2024</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th style=\"text-align: left;\">Description</th>\n",
    "<td style=\"text-align: left;\">Optimal Value Functions</td>\n",
    "</tr>\n",
    "</table>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de54d763",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "In this task, we are working with a Markov Decision Process (MDP) where:\n",
    "- **States**: \\( $S_0$, $S_1$, $S_2$, $S_3$ \\)\n",
    "- **Actions**: \\( $a_0$, $a_1$, $a_2$ \\)\n",
    "- A **policy** ( $\\pi$ \\) defines the action to take for each state. For example, a policy might specify:\n",
    "  - Take \\( $a_0$ \\) in \\( $S_0$ \\),\n",
    "  - Take \\( $a_1$ \\) in \\( $S_1$ \\),\n",
    "  - Take \\( $a_2$ \\) in \\( $S_2$ \\),\n",
    "  - Take \\( $a_0$ \\) in \\( $S_3$ \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899fea18",
   "metadata": {},
   "source": [
    "We first import the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7524485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259294c",
   "metadata": {},
   "source": [
    "## Question 1 : Enumerate all the possible policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4eb5f",
   "metadata": {},
   "source": [
    "The objective is to enumerate all possible policies. Since each state can choose one of three actions (\\( $a_0$, $a_1$, $a_2$ \\)), the total number of possible policies is:\n",
    "\\[\n",
    "$3^4 = 81$\n",
    "\\]\n",
    "\n",
    "### Approach\n",
    "To systematically enumerate all possible policies:\n",
    "1. Define the set of states ( $S_0$, $S_1$, $S_2$, $S_3$ \\) and actions \\( $a_0$, $a_1$, $a_2$ \\).\n",
    "2. Use the Cartesian product of actions repeated for the number of states to generate all combinations of actions for the states.\n",
    "3. Output each policy in a readable format, mapping states to actions.\n",
    "\n",
    "The code below implements this approach, generating and displaying all possible policies. Additionally, it saves the policies to a CSV file for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d6bb47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of policies: 81\n",
      "\n",
      "Policy 1: {'S0': 'a0', 'S1': 'a0', 'S2': 'a0', 'S3': 'a0'}\n",
      "Policy 2: {'S0': 'a0', 'S1': 'a0', 'S2': 'a0', 'S3': 'a1'}\n",
      "Policy 3: {'S0': 'a0', 'S1': 'a0', 'S2': 'a0', 'S3': 'a2'}\n",
      "Policy 4: {'S0': 'a0', 'S1': 'a0', 'S2': 'a1', 'S3': 'a0'}\n",
      "Policy 5: {'S0': 'a0', 'S1': 'a0', 'S2': 'a1', 'S3': 'a1'}\n",
      "Policy 6: {'S0': 'a0', 'S1': 'a0', 'S2': 'a1', 'S3': 'a2'}\n",
      "Policy 7: {'S0': 'a0', 'S1': 'a0', 'S2': 'a2', 'S3': 'a0'}\n",
      "Policy 8: {'S0': 'a0', 'S1': 'a0', 'S2': 'a2', 'S3': 'a1'}\n",
      "Policy 9: {'S0': 'a0', 'S1': 'a0', 'S2': 'a2', 'S3': 'a2'}\n",
      "Policy 10: {'S0': 'a0', 'S1': 'a1', 'S2': 'a0', 'S3': 'a0'}\n",
      "Policy 11: {'S0': 'a0', 'S1': 'a1', 'S2': 'a0', 'S3': 'a1'}\n",
      "Policy 12: {'S0': 'a0', 'S1': 'a1', 'S2': 'a0', 'S3': 'a2'}\n",
      "Policy 13: {'S0': 'a0', 'S1': 'a1', 'S2': 'a1', 'S3': 'a0'}\n",
      "Policy 14: {'S0': 'a0', 'S1': 'a1', 'S2': 'a1', 'S3': 'a1'}\n",
      "Policy 15: {'S0': 'a0', 'S1': 'a1', 'S2': 'a1', 'S3': 'a2'}\n",
      "Policy 16: {'S0': 'a0', 'S1': 'a1', 'S2': 'a2', 'S3': 'a0'}\n",
      "Policy 17: {'S0': 'a0', 'S1': 'a1', 'S2': 'a2', 'S3': 'a1'}\n",
      "Policy 18: {'S0': 'a0', 'S1': 'a1', 'S2': 'a2', 'S3': 'a2'}\n",
      "Policy 19: {'S0': 'a0', 'S1': 'a2', 'S2': 'a0', 'S3': 'a0'}\n",
      "Policy 20: {'S0': 'a0', 'S1': 'a2', 'S2': 'a0', 'S3': 'a1'}\n",
      "Policy 21: {'S0': 'a0', 'S1': 'a2', 'S2': 'a0', 'S3': 'a2'}\n",
      "Policy 22: {'S0': 'a0', 'S1': 'a2', 'S2': 'a1', 'S3': 'a0'}\n",
      "Policy 23: {'S0': 'a0', 'S1': 'a2', 'S2': 'a1', 'S3': 'a1'}\n",
      "Policy 24: {'S0': 'a0', 'S1': 'a2', 'S2': 'a1', 'S3': 'a2'}\n",
      "Policy 25: {'S0': 'a0', 'S1': 'a2', 'S2': 'a2', 'S3': 'a0'}\n",
      "Policy 26: {'S0': 'a0', 'S1': 'a2', 'S2': 'a2', 'S3': 'a1'}\n",
      "Policy 27: {'S0': 'a0', 'S1': 'a2', 'S2': 'a2', 'S3': 'a2'}\n",
      "Policy 28: {'S0': 'a1', 'S1': 'a0', 'S2': 'a0', 'S3': 'a0'}\n",
      "Policy 29: {'S0': 'a1', 'S1': 'a0', 'S2': 'a0', 'S3': 'a1'}\n",
      "Policy 30: {'S0': 'a1', 'S1': 'a0', 'S2': 'a0', 'S3': 'a2'}\n",
      "Policy 31: {'S0': 'a1', 'S1': 'a0', 'S2': 'a1', 'S3': 'a0'}\n",
      "Policy 32: {'S0': 'a1', 'S1': 'a0', 'S2': 'a1', 'S3': 'a1'}\n",
      "Policy 33: {'S0': 'a1', 'S1': 'a0', 'S2': 'a1', 'S3': 'a2'}\n",
      "Policy 34: {'S0': 'a1', 'S1': 'a0', 'S2': 'a2', 'S3': 'a0'}\n",
      "Policy 35: {'S0': 'a1', 'S1': 'a0', 'S2': 'a2', 'S3': 'a1'}\n",
      "Policy 36: {'S0': 'a1', 'S1': 'a0', 'S2': 'a2', 'S3': 'a2'}\n",
      "Policy 37: {'S0': 'a1', 'S1': 'a1', 'S2': 'a0', 'S3': 'a0'}\n",
      "Policy 38: {'S0': 'a1', 'S1': 'a1', 'S2': 'a0', 'S3': 'a1'}\n",
      "Policy 39: {'S0': 'a1', 'S1': 'a1', 'S2': 'a0', 'S3': 'a2'}\n",
      "Policy 40: {'S0': 'a1', 'S1': 'a1', 'S2': 'a1', 'S3': 'a0'}\n",
      "Policy 41: {'S0': 'a1', 'S1': 'a1', 'S2': 'a1', 'S3': 'a1'}\n",
      "Policy 42: {'S0': 'a1', 'S1': 'a1', 'S2': 'a1', 'S3': 'a2'}\n",
      "Policy 43: {'S0': 'a1', 'S1': 'a1', 'S2': 'a2', 'S3': 'a0'}\n",
      "Policy 44: {'S0': 'a1', 'S1': 'a1', 'S2': 'a2', 'S3': 'a1'}\n",
      "Policy 45: {'S0': 'a1', 'S1': 'a1', 'S2': 'a2', 'S3': 'a2'}\n",
      "Policy 46: {'S0': 'a1', 'S1': 'a2', 'S2': 'a0', 'S3': 'a0'}\n",
      "Policy 47: {'S0': 'a1', 'S1': 'a2', 'S2': 'a0', 'S3': 'a1'}\n",
      "Policy 48: {'S0': 'a1', 'S1': 'a2', 'S2': 'a0', 'S3': 'a2'}\n",
      "Policy 49: {'S0': 'a1', 'S1': 'a2', 'S2': 'a1', 'S3': 'a0'}\n",
      "Policy 50: {'S0': 'a1', 'S1': 'a2', 'S2': 'a1', 'S3': 'a1'}\n",
      "Policy 51: {'S0': 'a1', 'S1': 'a2', 'S2': 'a1', 'S3': 'a2'}\n",
      "Policy 52: {'S0': 'a1', 'S1': 'a2', 'S2': 'a2', 'S3': 'a0'}\n",
      "Policy 53: {'S0': 'a1', 'S1': 'a2', 'S2': 'a2', 'S3': 'a1'}\n",
      "Policy 54: {'S0': 'a1', 'S1': 'a2', 'S2': 'a2', 'S3': 'a2'}\n",
      "Policy 55: {'S0': 'a2', 'S1': 'a0', 'S2': 'a0', 'S3': 'a0'}\n",
      "Policy 56: {'S0': 'a2', 'S1': 'a0', 'S2': 'a0', 'S3': 'a1'}\n",
      "Policy 57: {'S0': 'a2', 'S1': 'a0', 'S2': 'a0', 'S3': 'a2'}\n",
      "Policy 58: {'S0': 'a2', 'S1': 'a0', 'S2': 'a1', 'S3': 'a0'}\n",
      "Policy 59: {'S0': 'a2', 'S1': 'a0', 'S2': 'a1', 'S3': 'a1'}\n",
      "Policy 60: {'S0': 'a2', 'S1': 'a0', 'S2': 'a1', 'S3': 'a2'}\n",
      "Policy 61: {'S0': 'a2', 'S1': 'a0', 'S2': 'a2', 'S3': 'a0'}\n",
      "Policy 62: {'S0': 'a2', 'S1': 'a0', 'S2': 'a2', 'S3': 'a1'}\n",
      "Policy 63: {'S0': 'a2', 'S1': 'a0', 'S2': 'a2', 'S3': 'a2'}\n",
      "Policy 64: {'S0': 'a2', 'S1': 'a1', 'S2': 'a0', 'S3': 'a0'}\n",
      "Policy 65: {'S0': 'a2', 'S1': 'a1', 'S2': 'a0', 'S3': 'a1'}\n",
      "Policy 66: {'S0': 'a2', 'S1': 'a1', 'S2': 'a0', 'S3': 'a2'}\n",
      "Policy 67: {'S0': 'a2', 'S1': 'a1', 'S2': 'a1', 'S3': 'a0'}\n",
      "Policy 68: {'S0': 'a2', 'S1': 'a1', 'S2': 'a1', 'S3': 'a1'}\n",
      "Policy 69: {'S0': 'a2', 'S1': 'a1', 'S2': 'a1', 'S3': 'a2'}\n",
      "Policy 70: {'S0': 'a2', 'S1': 'a1', 'S2': 'a2', 'S3': 'a0'}\n",
      "Policy 71: {'S0': 'a2', 'S1': 'a1', 'S2': 'a2', 'S3': 'a1'}\n",
      "Policy 72: {'S0': 'a2', 'S1': 'a1', 'S2': 'a2', 'S3': 'a2'}\n",
      "Policy 73: {'S0': 'a2', 'S1': 'a2', 'S2': 'a0', 'S3': 'a0'}\n",
      "Policy 74: {'S0': 'a2', 'S1': 'a2', 'S2': 'a0', 'S3': 'a1'}\n",
      "Policy 75: {'S0': 'a2', 'S1': 'a2', 'S2': 'a0', 'S3': 'a2'}\n",
      "Policy 76: {'S0': 'a2', 'S1': 'a2', 'S2': 'a1', 'S3': 'a0'}\n",
      "Policy 77: {'S0': 'a2', 'S1': 'a2', 'S2': 'a1', 'S3': 'a1'}\n",
      "Policy 78: {'S0': 'a2', 'S1': 'a2', 'S2': 'a1', 'S3': 'a2'}\n",
      "Policy 79: {'S0': 'a2', 'S1': 'a2', 'S2': 'a2', 'S3': 'a0'}\n",
      "Policy 80: {'S0': 'a2', 'S1': 'a2', 'S2': 'a2', 'S3': 'a1'}\n",
      "Policy 81: {'S0': 'a2', 'S1': 'a2', 'S2': 'a2', 'S3': 'a2'}\n"
     ]
    }
   ],
   "source": [
    "# Define states and actions\n",
    "states = ['S0', 'S1', 'S2', 'S3']\n",
    "actions = ['a0', 'a1', 'a2']\n",
    "\n",
    "# Create all policies as combinations of actions for the states\n",
    "policies = list(product(actions, repeat=len(states)))\n",
    "\n",
    "# Display all policies\n",
    "print(f\"Total number of policies: {len(policies)}\\n\")\n",
    "for i, policy in enumerate(policies, 1):\n",
    "    policy_dict = {state: action for state, action in zip(states, policy)}\n",
    "    print(f\"Policy {i}: {policy_dict}\")\n",
    "\n",
    "# Saved on a csv file : policies.csv\n",
    "policies_df = pd.DataFrame(policies, columns=states)\n",
    "policies_df.to_csv(\"policies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141a76d",
   "metadata": {},
   "source": [
    "## Question 2 : optimal value function \\( $V^*(s)$ \\) for each state \\( $S_0$, $S_1$, $S_2$, $S_3$ \\)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f3f48",
   "metadata": {},
   "source": [
    "We need to compute the optimal value function \\( $V^*(s)$ \\) for each state \\( $S_0$, $S_1$, $S_2$, $S_3$ \\). The value function is given by:\n",
    "\n",
    "\\[\n",
    "$V^*(S) = R(S) + \\max_a \\gamma \\sum_{S'} T(S, a, S') V^*(S')$\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( R(S) \\) is the reward for state \\( S \\),\n",
    "- \\( T(S, a, S') \\) is the transition probability from state \\( S \\) to state \\( S' \\) under action \\( a \\),\n",
    "- \\( $\\gamma$ \\) is the discount factor.\n",
    "\n",
    "\n",
    "### Equations for Each State\n",
    "\n",
    "#### For \\( $S_0$ \\):\n",
    "\n",
    "$V^*(S_0) = \\max_a \\gamma \\Big( T(S_0, a, S_1) V^*(S_1) + T(S_0, a, S_2) V^*(S_2) + T(S_0, a, S_3) V^*(S_3) \\Big)$\n",
    "\n",
    "#### For \\( $S_1$ \\):\n",
    "\n",
    "$V^*(S_1) = \\max_a \\gamma \\Big( T(S_1, a, S_1) V^*(S_1) + T(S_1, a, S_3) V^*(S_3) \\Big)$\n",
    "\n",
    "\n",
    "#### For \\( $S_2$ \\):\n",
    "\n",
    "$V^*(S_2) = 1 + \\max_a \\gamma \\Big( T(S_2, a, S_0) V^*(S_0) \\Big)$\n",
    "\n",
    "\n",
    "#### For \\( $S_3$ \\):\n",
    "\n",
    "$V^*(S_3) = 10$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adca0a5",
   "metadata": {},
   "source": [
    "## Question 3 : Is there a value for  $x$ such that $\\pi^*(S_0) = a_2$ for all $\\gamma \\in [0, 1]$ and $y \\in [0, 1]$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e736689",
   "metadata": {},
   "source": [
    "We need to determine whether there exists a value for  $x$ such that the optimal policy for $S_0$ is always $a_2$, regardless of the values of $\\gamma$ and $y$, where:\n",
    "- $\\gamma \\in [0, 1]$: The discount factor.\n",
    "- $y \\in [0, 1]$: A parameter influencing the transition probabilities.\n",
    "\n",
    "The policy $\\pi^*(S)$ is defined as:\n",
    "\n",
    "$\\pi^*(S) = arg\\max_a \\sum_{S'} T(S, a, S') V^*(S')$\n",
    "\n",
    "\n",
    "The value function $V^*(S)$ is given by:\n",
    "\n",
    "$V^*(S) = R(S) + \\max_a \\gamma \\sum_{S'} T(S, a, S') V^*(S')$\n",
    "\n",
    "Where:\n",
    "- $R(S)$: The reward for being in state $S$,\n",
    "- $T(S, a, S')$: The transition probability from $S$ to $S'$ under action $a$,\n",
    "- $\\gamma$: The discount factor.\n",
    "\n",
    "### Analyzing $\\pi^*(S_0)$ for $a_2$\n",
    "\n",
    "#### Transition Probabilities\n",
    "From the matrix $T(S_0, a_2, S')$, the only non-zero probability is $T(S_0, a_2, S_2) = 1$. This means if $a_2$ is taken in $S_0$, the agent will always transition to $S_2$.\n",
    "\n",
    "#### Value Function for $a_2$\n",
    "If $a_2$ is taken in $S_0$, the value function $V^*(S_0)$ will depend on the value of $S_2$:\n",
    "\n",
    "$V^*(S_0 \\mid a_2) = \\gamma V^*(S_2)$\n",
    "\n",
    "Using the value function for $S_2$:\n",
    "\n",
    "$V^*(S_2) = 1 + \\max_a \\gamma T(S_2, a, S_0) V^*(S_0)$\n",
    "\n",
    "### Comparing Actions \\( $a_0$, $a_1$, $a_2$ \\)\n",
    "\n",
    "1. **For $a_0$:**\n",
    "   The value function for $S_0$ under $a_0$ involves terms $x V^*(S_3)$ and $y V^*(S_2)$:\n",
    "   \n",
    "   $V^*(S_0 \\mid a_0) = \\gamma \\Big( (1 - y) V^*(S_1) + x V^*(S_3) + y V^*(S_2) \\Big)$\n",
    "   \n",
    "\n",
    "2. **For $a_1$:**\n",
    "   $a_1$ always transitions to $S_3$:\n",
    "   \n",
    "   $V^*(S_0 \\mid a_1) = \\gamma V^*(S_3)$\n",
    "   \n",
    "   Substituting $V^*(S_3) = 10$, we get:\n",
    "   \n",
    "   $V^*(S_0 \\mid a_1) = 10 \\gamma$\n",
    "   \n",
    "\n",
    "3. **For $a_2$:**\n",
    "   As discussed earlier:\n",
    "   \n",
    "   $V^*(S_0 \\mid a_2) = \\gamma V^*(S_2)$\n",
    "   \n",
    "\n",
    "### Condition for $\\pi^*(S_0) = a_2$\n",
    "\n",
    "For $\\pi^*(S_0) = a_2$, the value of $V^*(S_0 \\mid a_2)$ must be greater than or equal to the values for $a_0$ and $a_1$:\n",
    "\n",
    "$\\gamma V^*(S_2) \\geq \\max\\Big(\\gamma \\Big((1 - y) V^*(S_1) + x V^*(S_3) + y V^*(S_2)\\Big), 10 \\gamma\\Big)$\n",
    "\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Yes, it is possible to choose a value of $x$ such that $\\pi^*(S_0) = a_2$ for all $\\gamma \\in [0, 1]$ and $y \\in [0, 1]$. The value of $x$ must ensure that the term $x V^*(S_3)$ in $a_0$'s value function does not dominate, allowing $a_2$ to remain optimal. For example, setting $x = 0$ would eliminate $a_0$'s advantage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a5714",
   "metadata": {},
   "source": [
    "## Question 4: Is there a value for $y$ such that $\\pi^*(S_0) = a_1$ for all $x > 0$ and $y \\in [0, 1]$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5fb6d2",
   "metadata": {},
   "source": [
    "We need to determine whether there exists a value for $y$ such that the optimal policy for $S_0$ is always $a_1$, regardless of the values of $x > 0$ and $\\gamma \\in [0, 1]$. \n",
    "\n",
    "The policy $\\pi^*(S_0)$ is defined as:\n",
    "\n",
    "$\\pi^*(S_0) = \u0007rg\\max_a \\sum_{S'} T(S_0, a, S') V^*(S')$\n",
    "\n",
    "The value function $V^*(S_0)$ is given by:\n",
    "\n",
    "$V^*(S_0) = R(S_0) + \\max_a \\gamma \\sum_{S'} T(S_0, a, S') V^*(S')$\n",
    "\n",
    "Where:\n",
    "- $R(S_0)$: The reward for state $S_0$,\n",
    "- $T(S_0, a, S')$: The transition probability from $S_0$ to $S'$ under action $a$,\n",
    "- $\\gamma$: The discount factor.\n",
    "\n",
    "### Transition Probabilities and Value Analysis\n",
    "\n",
    "#### For Action $a_1$:\n",
    "The transition matrix shows that taking $a_1$ in $S_0$ always leads to $S_3$:\n",
    "\n",
    "$V^*(S_0 \\mid a_1) = \\gamma V^*(S_3)$\n",
    "\n",
    "Since $V^*(S_3) = 10$, we get:\n",
    "\n",
    "$V^*(S_0 \\mid a_1) = 10 \\gamma$\n",
    "\n",
    "#### For Action $a_0$:\n",
    "The value function for $S_0$ under $a_0$ involves terms $x V^*(S_3)$ and $y V^*(S_2)$:\n",
    "\n",
    "$V^*(S_0 \\mid a_0) = \\gamma \\Big((1 - y) V^*(S_1) + y V^*(S_2) + x V^*(S_3)\\Big)$\n",
    "\n",
    "#### For Action $a_2$:\n",
    "As derived earlier, $a_2$ always leads to $S_2$:\n",
    "\n",
    "$V^*(S_0 \\mid a_2) = \\gamma V^*(S_2)$\n",
    "\n",
    "### Condition for $\\pi^*(S_0) = a_1$\n",
    "\n",
    "For $\\pi^*(S_0) = a_1$, the value of $V^*(S_0 \\mid a_1)$ must be greater than or equal to the values for $a_0$ and $a_2$. This gives two conditions:\n",
    "\n",
    "#### 1. $V^*(S_0 \\mid a_1) \\geq V^*(S_0 \\mid a_0)$:\n",
    "\n",
    "$10 \\gamma \\geq \\gamma \\Big((1 - y) V^*(S_1) + y V^*(S_2) + x V^*(S_3)\\Big)$\n",
    "\n",
    "Dividing through by $\\gamma$ (assuming \\( $\\gamma$ > 0 \\)):\n",
    "\n",
    "$10 \\geq (1 - y) V^*(S_1) + y V^*(S_2) + x V^*(S_3)$\n",
    "\n",
    "##### 2. $V^*(S_0 \\mid a_1) \\geq V^*(S_0 \\mid a_2)$:\n",
    "\n",
    "$10 \\gamma \\geq \\gamma V^*(S_2)$\n",
    "\n",
    "Dividing through by $\\gamma$:\n",
    "\n",
    "$10 \\geq V^*(S_2)$\n",
    "\n",
    "\n",
    "### Finding Conditions on \\( y \\)\n",
    "\n",
    "- From Condition 2:\n",
    "  $V^*(S_2) \\leq 10$, which is always true since $R(S_2) = 1$ and $V^*(S_2)$ depends on the discounted future rewards.\n",
    "  \n",
    "- From Condition 1:\n",
    "\n",
    "$10 \\geq (1 - y) V^*(S_1) + y V^*(S_2) + x V^*(S_3)$\n",
    "\n",
    "Substituting $V^*(S_3) = 10$ and assuming $x > 0$:\n",
    "\n",
    "$10 \\geq (1 - y) V^*(S_1) + y V^*(S_2) + 10x$\n",
    "\n",
    "For $x > 0$, $10x$ increases. To ensure $a_1$ remains optimal, $y$ must be chosen to reduce the influence of $y V^*(S_2)$ and $10x$. Specifically, $y$ needs to minimize $y V^*(S_2)$.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Yes, there exists a value for $y$ such that $\\pi^*(S_0) = a_1$ for all $x > 0$. By setting $y = 0$, the contribution of $y V^*(S_2)$ is eliminated, ensuring that $V^*(S_0 \\mid a_1)$ is greater than $V^*(S_0 \\mid a_0)$ and $V^*(S_0 \\mid a_2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dfef60",
   "metadata": {},
   "source": [
    "## Question 5 : Value Iteration Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d02fb",
   "metadata": {},
   "source": [
    "We need to compute the optimal values $V^*$ and policies $\\pi^*$ for all states in a Markov Decision Process (MDP) using **value iteration**. The following parameters are provided:\n",
    "- $x = y = 0.25$\n",
    "- $\\gamma = 0.9$ (discount factor)\n",
    "\n",
    "The value iteration algorithm uses the following update equations:\n",
    "1. **Q-Value Update**:\n",
    "\n",
    "$Q_{k+1}(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s' | s, a) V_k(s')$\n",
    "\n",
    "2. **Value Function Update**:\n",
    "\n",
    "$V_k(s) = \\max_a Q_k(s, a)$\n",
    "\n",
    "The termination rule is:\n",
    "\n",
    "$|V_k(S) - V_{k-1}(S)| < 0.0001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25104ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define states and actions\n",
    "states = ['S0', 'S1', 'S2', 'S3']\n",
    "actions = ['a0', 'a1', 'a2']\n",
    "\n",
    "# Define parameters\n",
    "gamma = 0.9  # Discount factor\n",
    "x = 0.25  # Parameter x\n",
    "y = 0.25  # Parameter y\n",
    "epsilon = 0.0001  # Convergence threshold\n",
    "\n",
    "# Reward function\n",
    "rewards = {\n",
    "    'S0': 0,\n",
    "    'S1': 0,\n",
    "    'S2': 1,\n",
    "    'S3': 10\n",
    "}\n",
    "\n",
    "# Transition probabilities\n",
    "transition_probabilities = {\n",
    "    ('S0', 'a0', 'S1'): 1 - y,\n",
    "    ('S0', 'a0', 'S2'): y,\n",
    "    ('S0', 'a0', 'S3'): x,\n",
    "    ('S0', 'a1', 'S3'): 1,\n",
    "    ('S0', 'a2', 'S2'): 1,\n",
    "    ('S1', 'a0', 'S1'): 1,\n",
    "    ('S2', 'a0', 'S0'): 1,\n",
    "    ('S3', 'a0', 'S3'): 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fecc3ee",
   "metadata": {},
   "source": [
    "### Steps to Solve\n",
    "1. **Initialize**: Set the value function $V(S)$ for all states to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59c59d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize value function\n",
    "V = {state: 0.0 for state in states}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419507d3",
   "metadata": {},
   "source": [
    "2. **Iterative Updates**:\n",
    "   - For each state $S$, calculate $Q(s, a)$ for all actions $a$.\n",
    "   - Update $V(S)$ using the maximum $Q(s, a)$.\n",
    "   - Repeat until the values converge (difference between successive iterations is below the threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c7d9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get transition probability\n",
    "def get_transition_prob(s, a, s_prime):\n",
    "    return transition_probabilities.get((s, a, s_prime), 0)\n",
    "\n",
    "# Value iteration algorithm\n",
    "def value_iteration():\n",
    "    global V\n",
    "    delta = float('inf')\n",
    "    while delta > epsilon:\n",
    "        delta = 0\n",
    "        new_V = V.copy()\n",
    "        for s in states:\n",
    "            max_value = float('-inf')\n",
    "            for a in actions:\n",
    "                value = rewards[s] + gamma * sum(\n",
    "                    get_transition_prob(s, a, s_prime) * V[s_prime] for s_prime in states\n",
    "                )\n",
    "                max_value = max(max_value, value)\n",
    "            new_V[s] = max_value\n",
    "            delta = max(delta, abs(new_V[s] - V[s]))\n",
    "        V = new_V\n",
    "    return V\n",
    "\n",
    "# Run value iteration\n",
    "optimal_values = value_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43672623",
   "metadata": {},
   "source": [
    "3. **Optimal Policy**:\n",
    "   - Derive the optimal action for each state based on the final $Q(s, a)$ values.\n",
    "\n",
    "The implementation below calculates $V^*$ and $\\pi^*$ using the given parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3092498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the optimal policy\n",
    "optimal_policy = {}\n",
    "for s in states:\n",
    "    best_action = None\n",
    "    best_value = float('-inf')\n",
    "    for a in actions:\n",
    "        value = rewards[s] + gamma * sum(\n",
    "            get_transition_prob(s, a, s_prime) * V[s_prime] for s_prime in states\n",
    "        )\n",
    "        if value > best_value:\n",
    "            best_value = value\n",
    "            best_action = a\n",
    "    optimal_policy[s] = best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff1802d",
   "metadata": {},
   "source": [
    "And then we can display the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31613de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Values (V*):\n",
      "V*(S0) = 89.99916647515822\n",
      "V*(S1) = 0.0\n",
      "V*(S2) = 81.99916647515822\n",
      "V*(S3) = 99.99916647515822\n",
      "Optimal Policy (π*):\n",
      "π*(S0) = a1\n",
      "π*(S1) = a0\n",
      "π*(S2) = a0\n",
      "π*(S3) = a0\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"Optimal Values (V*):\")\n",
    "for state in states:\n",
    "    print(f\"V*({state}) = {V[state]}\")\n",
    "\n",
    "print(\"Optimal Policy (π*):\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"π*({state}) = {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daba494c",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this document, we tackled the problem of solving a Markov Decision Process (MDP) using value iteration to compute the optimal policy $\\pi^*$ and optimal value function $V^*$. Here’s a summary of the steps and insights:\n",
    "\n",
    "1. **Enumeration of Policies**:\n",
    "   - We systematically enumerated all possible policies for the given states and actions, highlighting the exhaustive nature of policy-based approaches.\n",
    "   - This provided a foundation for understanding the optimization process.\n",
    "\n",
    "2. **Optimal Value Function Equations**:\n",
    "   - The equations for $V^*(S_0), V^*(S_1), V^*(S_2), V^*(S_3)$ were derived based on the reward function $R(S)$, transition probabilities $T(S, a, S')$, and the discount factor $\\gamma$.\n",
    "   - These equations formed the basis of our iterative approach in value iteration.\n",
    "\n",
    "3. **Analysis of Policies for Specific Parameters**:\n",
    "   - We analyzed whether specific values of parameters $x$ and $y$ ensured optimality of certain actions ($\\pi^*(S_0) = a_2$ or $\\pi^*(S_0) = a_1$).\n",
    "   - This step demonstrated the role of parameter tuning in influencing policy decisions.\n",
    "\n",
    "4. **Value Iteration Implementation**:\n",
    "   - Using Python, we implemented the value iteration algorithm to compute $V^*$ and $\\pi^*$.\n",
    "   - For $x = y = 0.25$ and $\\gamma = 0.9$, the algorithm converged to a solution with a threshold $\\epsilon = 0.0001$.\n",
    "   - The optimal policy and values were derived, showing the power of iterative methods in solving MDPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6dc13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
